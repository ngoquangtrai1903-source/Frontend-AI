import joblib
import pandas as pd
import shap
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from google import genai
from contextlib import asynccontextmanager

# --- 1. C·∫§U H√åNH AI & BI·∫æN TO√ÄN C·ª§C ---
GEMINI_API_KEY = "AIzaSyBrdxlHX0E6GgUkv94hDBJhg2Ewk60f4JA"
GEMINI_CLIENT = genai.Client(api_key=GEMINI_API_KEY)
GEMINI_MODEL_ID = "gemini-2.0-flash"

MODELS = {}

# --- 2. QU·∫¢N L√ù V√íNG ƒê·ªúI (LIFESPAN) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # [STARTUP]: Ch·∫°y khi server b·∫Øt ƒë·∫ßu
    try:
        # ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi nh∆∞ b·∫°n ƒë√£ thi·∫øt l·∫≠p
        MODELS['clinical_model'] = joblib.load(r'D:\laptrinhpython\Final\diabetes_model.pkl')
        MODELS['clinical_scaler'] = joblib.load(r'D:\laptrinhpython\Final\scaler_diabetes.pkl')
        MODELS['clinical_encoders'] = joblib.load(r'D:\laptrinhpython\Final\label_encoders.pkl')
        MODELS['clinical_background'] = joblib.load(r'D:\laptrinhpython\Final\x_train_sample.pkl')

        MODELS['home_model'] = joblib.load(r'D:\laptrinhpython\Final\diabetes_model_home.pkl')
        MODELS['home_background'] = joblib.load(r'D:\laptrinhpython\Final\x_train_sample_home.pkl')

        print("‚úÖ [Lifespan] ƒê√£ t·∫£i t·∫•t c·∫£ Models th√†nh c√¥ng!")
    except Exception as e:
        print(f"‚ùå [Lifespan] L·ªói t·∫£i model: {e}")

    yield
    MODELS.clear()
    print("üßπ [Lifespan] ƒê√£ gi·∫£i ph√≥ng b·ªô nh·ªõ.")

# --- 3. KH·ªûI T·∫†O APP & CORS ---
app = FastAPI(title="DiabeTwin AI Backend", lifespan=lifespan)

# FIXED: Corrected the syntax error - was "a   pp.add_middleware"
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Cho ph√©p t·∫•t c·∫£ c√°c ngu·ªìn (Next.js port 3000)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 4. SCHEMA D·ªÆ LI·ªÜU ---
class ClinicalInput(BaseModel):
    gender: str
    age: int
    smoking_history: str
    hypertension: int
    heart_disease: int
    bmi: float
    hba1c: float
    glucose: int

class HomeInput(BaseModel):
    HighBP: int
    HighChol: int
    CholCheck: int
    BMI: float
    Smoker: int
    Stroke: int
    HeartDiseaseorAttack: int
    PhysActivity: int
    Fruits: int
    Veggies: int
    HvyAlcoholConsump: int
    GenHlth: int
    MentHlth: int
    PhysHlth: int
    DiffWalk: int
    Sex: int
    Age: int

# --- 5. ENDPOINTS ---

@app.post("/api/predict/clinical")
async def predict_clinical(data: ClinicalInput):
    try:
        encoders = MODELS['clinical_encoders']
        scaler = MODELS['clinical_scaler']

        input_list = [
            encoders['gender'].transform([data.gender])[0],
            data.age, data.hypertension, data.heart_disease,
            encoders['smoking_history'].transform([data.smoking_history])[0],
            data.bmi, data.hba1c, data.glucose
        ]

        df = pd.DataFrame([input_list], columns=MODELS['clinical_background'].columns)
        scaled_df = pd.DataFrame(scaler.transform(df), columns=df.columns)

        prob = float(MODELS['clinical_model'].predict_proba(scaled_df)[0][1])

        # SHAP Calculation
        f = lambda x: MODELS['clinical_model'].predict_proba(x)[:, 1]
        background = scaler.transform(MODELS['clinical_background'].sample(100))
        explainer = shap.Explainer(f, background)
        shap_values = explainer(scaled_df)

        impacts = []
        features = ["Gi·ªõi t√≠nh", "Tu·ªïi", "Huy·∫øt √°p", "B·ªánh tim", "H√∫t thu·ªëc", "BMI", "HbA1c", "ƒê∆∞·ªùng huy·∫øt"]
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": features[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "B√°c sƒ©", data.dict())

        return {
            "probability": round(prob * 100, 2),
            "status": "D∆Ø∆†NG T√çNH" if prob > 0.5 else "√ÇM T√çNH",
            "risk_level": "üî¥" if prob > 0.7 else "üü°" if prob > 0.3 else "üü¢",
            "impacts": impacts,
            "ai_advice": advice
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/predict/home")
async def predict_home(data: HomeInput):
    try:
        df = pd.DataFrame([data.dict()])
        prob = float(MODELS['home_model'].predict_proba(df)[0][1])

        f = lambda x: MODELS['home_model'].predict_proba(x)[:, 1]
        explainer = shap.Explainer(f, MODELS['home_background'])
        shap_values = explainer(df)

        display_names = [
            "Huy·∫øt √°p cao", "Cholesterol cao", "Ki·ªÉm tra Chol", "Ch·ªâ s·ªë BMI",
            "H√∫t thu·ªëc", "ƒê·ªôt qu·ªµ", "B·ªánh tim", "V·∫≠n ƒë·ªông", "Tr√°i c√¢y",
            "Rau xanh", "R∆∞·ª£u bia", "S·ª©c kh·ªèe t·ªïng qu√°t", "S·ª©c kh·ªèe t√¢m th·∫ßn",
            "S·ª©c kh·ªèe th·ªÉ ch·∫•t", "ƒêi l·∫°i kh√≥", "Gi·ªõi t√≠nh", "Nh√≥m tu·ªïi"
        ]

        impacts = []
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": display_names[i], "impact": round(val * 100, 2)})

        advice = await get_gemini_advice(prob, impacts, "Ng∆∞·ªùi d√πng", data.dict())

        return {
            "probability": round(prob * 100, 2),
            "status": "NGUY C∆† CAO" if prob > 0.5 else "AN TO√ÄN",
            "impacts": impacts,
            "ai_advice": advice
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "models_loaded": len(MODELS) > 0
    }

async def get_gemini_advice(prob, impacts, role, raw_data):
    top_3 = sorted(impacts, key=lambda x: abs(x['impact']), reverse=True)[:3]
    top_3_str = "\n".join([f"- {i['feature']}: {i['impact']}%" for i in top_3])

    prompt = f"""
    B·∫°n l√† b√°c sƒ© t∆∞ v·∫•n AI. Ph√¢n t√≠ch k·∫øt qu·∫£ cho {role}.
    Nguy c∆°: {prob * 100:.1f}%.
    3 y·∫øu t·ªë ·∫£nh h∆∞·ªüng nh·∫•t t·ª´ SHAP: {top_3_str}
    D·ªØ li·ªáu th√¥: {raw_data}
    Gi·∫£i th√≠ch ng·∫Øn g·ªçn √Ω nghƒ©a c√°c con s·ªë % v√† ƒë∆∞a ra 3 l·ªùi khuy√™n.
    """
    try:
        res = GEMINI_CLIENT.models.generate_content(model=GEMINI_MODEL_ID, contents=prompt)
        return res.text
    except:
        return "T·∫°m th·ªùi kh√¥ng th·ªÉ k·∫øt n·ªëi t∆∞ v·∫•n AI."

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
