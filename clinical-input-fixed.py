import joblib
import pandas as pd
import shap
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from contextlib import asynccontextmanager
import logging
import os
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor

load_dotenv(override=True)

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 1. CONFIGURATION & GLOBALS ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_ID = "gemini-2.5-flash"
GEMINI_CLIENT = None
executor = ThreadPoolExecutor(max_workers=2)

print(f"DEBUG: Gemini API Key configured: {bool(GEMINI_API_KEY)}")

def init_gemini_client():
    """Initialize Gemini client with error handling"""
    global GEMINI_CLIENT
    try:
        import google.generativeai as genai
        genai.configure(api_key=GEMINI_API_KEY)
        GEMINI_CLIENT = genai
        logger.info("âœ… Gemini API client initialized successfully")
        return True
    except ImportError:
        logger.error("âŒ google-generativeai package not installed")
        logger.info("Install with: pip install google-generativeai")
        return False
    except Exception as e:
        logger.error(f"âŒ Failed to initialize Gemini client: {e}")
        return False


MODELS = {}


# --- 2. LIFECYCLE MANAGEMENT ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # [STARTUP]
    try:
        # Load ML models
        MODELS['clinical_model'] = joblib.load(r'D:\laptrinhpython\Final\diabetes_model.pkl')
        MODELS['clinical_scaler'] = joblib.load(r'D:\laptrinhpython\Final\scaler_diabetes.pkl')
        MODELS['clinical_encoders'] = joblib.load(r'D:\laptrinhpython\Final\label_encoders.pkl')
        MODELS['clinical_background'] = joblib.load(r'D:\laptrinhpython\Final\x_train_sample.pkl')

        MODELS['home_model'] = joblib.load(r'D:\laptrinhpython\Final\diabetes_model_home.pkl')
        MODELS['home_background'] = joblib.load(r'D:\laptrinhpython\Final\x_train_sample_home.pkl')

        logger.info("âœ… All ML Models loaded successfully!")

        # Initialize Gemini
        gemini_ok = init_gemini_client()
        if not gemini_ok:
            logger.warning("âš ï¸ Gemini API not available - will use fallback advice")

    except Exception as e:
        logger.error(f"âŒ Error loading models: {e}")
        raise

    yield
    MODELS.clear()
    logger.info("ðŸ§¹ Memory cleared on shutdown")


# --- 3. INITIALIZE APP & CORS ---
app = FastAPI(title="DiabeTwin AI Backend", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- 4. DATA SCHEMAS ---
class ClinicalInput(BaseModel):
    gender: str
    age: int
    smoking_history: str
    hypertension: int
    heart_disease: int
    bmi: float
    hba1c: float
    glucose: int


class HomeInput(BaseModel):
    HighBP: int
    HighChol: int
    CholCheck: int
    BMI: float
    Smoker: int
    Stroke: int
    HeartDiseaseorAttack: int
    PhysActivity: int
    Fruits: int
    Veggies: int
    HvyAlcoholConsump: int
    GenHlth: int
    MentHlth: int
    PhysHlth: int
    DiffWalk: int
    Sex: int
    Age: int


# --- 5. HELPER FUNCTIONS ---

def generate_fallback_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    """Generate default advice when Gemini is unavailable"""

    top_3 = sorted(impacts, key=lambda x: abs(x['impact']), reverse=True)[:3]
    risk_level = "cao" if prob > 0.7 else "trung bÃ¬nh" if prob > 0.4 else "tháº¥p"

    advice = f"""**PhÃ¢n tÃ­ch nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng**

ðŸŽ¯ **Káº¿t quáº£:** Nguy cÆ¡ á»Ÿ má»©c **{risk_level}** vá»›i xÃ¡c suáº¥t {prob * 100:.1f}%

ðŸ“Š **3 Yáº¿u tá»‘ áº£nh hÆ°á»Ÿng lá»›n nháº¥t:**
"""

    for i, impact in enumerate(top_3, 1):
        direction = "tÄƒng" if impact['impact'] > 0 else "giáº£m"
        advice += f"\n{i}. **{impact['feature']}**: {direction} {abs(impact['impact']):.1f}% nguy cÆ¡"

    advice += "\n\nðŸ’¡ **Khuyáº¿n nghá»‹:**\n"

    if prob > 0.7:
        advice += """
1. **Kháº©n cáº¥p:** Cáº§n gáº·p bÃ¡c sÄ© chuyÃªn khoa tiá»ƒu Ä‘Æ°á»ng trong vÃ²ng 1 tuáº§n
2. **Kiá»ƒm tra:** XÃ©t nghiá»‡m HbA1c vÃ  Ä‘Æ°á»ng huyáº¿t Ä‘Ã³i ngay
3. **Lá»‘i sá»‘ng:** Báº¯t Ä‘áº§u cháº¿ Ä‘á»™ Äƒn kiÃªng Ã­t Ä‘Æ°á»ng, tÄƒng váº­n Ä‘á»™ng ngay láº­p tá»©c
"""
    elif prob > 0.4:
        advice += """
1. **Theo dÃµi:** Äáº·t lá»‹ch khÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ 3-6 thÃ¡ng/láº§n
2. **PhÃ²ng ngá»«a:** Äiá»u chá»‰nh cháº¿ Ä‘á»™ Äƒn, tÄƒng váº­n Ä‘á»™ng 30 phÃºt/ngÃ y
3. **Kiá»ƒm tra:** Theo dÃµi cÃ¡c chá»‰ sá»‘ sá»©c khá»e táº¡i nhÃ 
"""
    else:
        advice += """
1. **Duy trÃ¬:** Tiáº¿p tá»¥c lá»‘i sá»‘ng lÃ nh máº¡nh hiá»‡n táº¡i
2. **Kiá»ƒm tra:** KhÃ¡m sá»©c khá»e Ä‘á»‹nh ká»³ hÃ ng nÄƒm
3. **PhÃ²ng ngá»«a:** Giá»¯ cÃ¢n náº·ng á»•n Ä‘á»‹nh, váº­n Ä‘á»™ng Ä‘á»u Ä‘áº·n
"""

    advice += "\n\nâš ï¸ *LÆ°u Ã½: Káº¿t quáº£ chá»‰ mang tÃ­nh tham kháº£o. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© chuyÃªn khoa.*"

    return advice


def call_gemini_sync(prompt: str) -> str:
    """Synchronous call to Gemini API with proper error handling"""
    if GEMINI_CLIENT is None:
        logger.warning("Gemini client not initialized")
        return None

    try:
        logger.info("Calling Gemini API...")
        
        # Use the correct API call method
        model = GEMINI_CLIENT.GenerativeModel(GEMINI_MODEL_ID)
        response = model.generate_content(
            prompt,
            generation_config=GEMINI_CLIENT.types.GenerationConfig(
                temperature=0.7,
                max_output_tokens=1000,
            )
        )
        
        if response and response.text:
            logger.info("âœ… Gemini API response received successfully")
            return response.text
        else:
            logger.warning("âš ï¸ Gemini API returned empty response")
            return None

    except Exception as e:
        logger.error(f"âŒ Gemini API error: {type(e).__name__}: {e}")
        return None


async def get_gemini_advice(prob: float, impacts: list, role: str, raw_data: dict) -> str:
    """Get advice from Gemini API with fallback and timeout"""

    if GEMINI_CLIENT is None:
        logger.warning("Using fallback advice (Gemini not available)")
        return generate_fallback_advice(prob, impacts, role, raw_data)

    top_3 = sorted(impacts, key=lambda x: abs(x['impact']), reverse=True)[:3]
    top_3_str = "\n".join([f"- {i['feature']}: {i['impact']:.1f}%" for i in top_3])

    prompt = f"""Báº¡n lÃ  bÃ¡c sÄ© tÆ° váº¥n AI chuyÃªn vá» tiá»ƒu Ä‘Æ°á»ng. PhÃ¢n tÃ­ch káº¿t quáº£ cho {role}.

**ThÃ´ng tin bá»‡nh nhÃ¢n:**
- Nguy cÆ¡ tiá»ƒu Ä‘Æ°á»ng: {prob * 100:.1f}%
- 3 yáº¿u tá»‘ áº£nh hÆ°á»Ÿng nháº¥t (SHAP values):
{top_3_str}

HÃ£y Ä‘Æ°a ra:
1. Giáº£i thÃ­ch ngáº¯n gá»n Ã½ nghÄ©a cÃ¡c con sá»‘ pháº§n trÄƒm
2. PhÃ¢n tÃ­ch nguy cÆ¡
3. 3-4 lá»i khuyÃªn cá»¥ thá»ƒ vÃ  kháº£ thi

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chuyÃªn nghiá»‡p nhÆ°ng dá»… hiá»ƒu."""

    try:
        # Run sync Gemini call in thread pool with timeout
        loop = asyncio.get_event_loop()
        response_text = await asyncio.wait_for(
            loop.run_in_executor(executor, call_gemini_sync, prompt),
            timeout=30.0  # 30 second timeout
        )

        if response_text:
            return response_text
        else:
            logger.warning("Gemini returned empty, using fallback")
            return generate_fallback_advice(prob, impacts, role, raw_data)

    except asyncio.TimeoutError:
        logger.error("âŒ Gemini API timeout (30s)")
        return generate_fallback_advice(prob, impacts, role, raw_data)
    except Exception as e:
        logger.error(f"âŒ Gemini API exception: {type(e).__name__}: {e}")
        return generate_fallback_advice(prob, impacts, role, raw_data)


# --- 6. ENDPOINTS ---

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "models_loaded": len(MODELS) > 0,
        "gemini_available": GEMINI_CLIENT is not None
    }


@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "DiabeTwin AI Backend",
        "version": "2.1",
        "endpoints": {
            "health": "/health",
            "clinical": "/api/predict/clinical",
            "home": "/api/predict/home",
            "docs": "/docs"
        }
    }


@app.post("/api/predict/clinical")
async def predict_clinical(data: ClinicalInput):
    """Clinical prediction endpoint (Doctor mode)"""
    try:
        logger.info(f"Clinical prediction request: {data.dict()}")

        encoders = MODELS['clinical_encoders']
        scaler = MODELS['clinical_scaler']

        input_list = [
            encoders['gender'].transform([data.gender])[0],
            data.age, data.hypertension, data.heart_disease,
            encoders['smoking_history'].transform([data.smoking_history])[0],
            data.bmi, data.hba1c, data.glucose
        ]

        df = pd.DataFrame([input_list], columns=MODELS['clinical_background'].columns)
        scaled_df = pd.DataFrame(scaler.transform(df), columns=df.columns)

        prob = float(MODELS['clinical_model'].predict_proba(scaled_df)[0][1])

        # SHAP Calculation
        f = lambda x: MODELS['clinical_model'].predict_proba(x)[:, 1]
        background = scaler.transform(MODELS['clinical_background'].sample(100))
        explainer = shap.Explainer(f, background)
        shap_values = explainer(scaled_df)

        impacts = []
        features = ["Giá»›i tÃ­nh", "Tuá»•i", "Huyáº¿t Ã¡p", "Bá»‡nh tim", "HÃºt thuá»‘c", "BMI", "HbA1c", "ÄÆ°á»ng huyáº¿t"]
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": features[i], "impact": round(val * 100, 2)})

        # Get AI advice with proper async handling
        advice = await get_gemini_advice(prob, impacts, "BÃ¡c sÄ©", data.dict())

        result = {
            "probability": round(prob * 100, 2),
            "status": "DÆ¯Æ NG TÃNH" if prob > 0.5 else "Ã‚M TÃNH",
            "risk_level": "ðŸ”´" if prob > 0.7 else "ðŸŸ¡" if prob > 0.3 else "ðŸŸ¢",
            "impacts": impacts,
            "ai_advice": advice
        }

        logger.info(f"Clinical prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Clinical prediction error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/predict/home")
async def predict_home(data: HomeInput):
    """Home prediction endpoint (User mode)"""
    try:
        logger.info(f"Home prediction request received")

        df = pd.DataFrame([data.dict()])
        prob = float(MODELS['home_model'].predict_proba(df)[0][1])

        f = lambda x: MODELS['home_model'].predict_proba(x)[:, 1]
        explainer = shap.Explainer(f, MODELS['home_background'])
        shap_values = explainer(df)

        display_names = [
            "Huyáº¿t Ã¡p cao", "Cholesterol cao", "Kiá»ƒm tra Chol", "Chá»‰ sá»‘ BMI",
            "HÃºt thuá»‘c", "Äá»™t quá»µ", "Bá»‡nh tim", "Váº­n Ä‘á»™ng", "TrÃ¡i cÃ¢y",
            "Rau xanh", "RÆ°á»£u bia", "Sá»©c khá»e tá»•ng quÃ¡t", "Sá»©c khá»e tÃ¢m tháº§n",
            "Sá»©c khá»e thá»ƒ cháº¥t", "Äi láº¡i khÃ³", "Giá»›i tÃ­nh", "NhÃ³m tuá»•i"
        ]

        impacts = []
        for i, val in enumerate(shap_values.values[0]):
            impacts.append({"feature": display_names[i], "impact": round(val * 100, 2)})

        # Get AI advice with proper async handling
        advice = await get_gemini_advice(prob, impacts, "NgÆ°á»i dÃ¹ng", data.dict())

        result = {
            "probability": round(prob * 100, 2),
            "status": "NGUY CÆ  CAO" if prob > 0.5 else "AN TOÃ€N",
            "impacts": impacts,
            "ai_advice": advice
        }

        logger.info(f"Home prediction successful: {prob * 100:.1f}%")
        return result

    except Exception as e:
        logger.error(f"Home prediction error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)
